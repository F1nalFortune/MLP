{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "from random import randint\n",
    "from time import time\n",
    "from IPython.core.display import clear_output\n",
    "from warnings import warn\n",
    "import math\n",
    "import cv2\n",
    "import os\n",
    "import unicodedata\n",
    "import string\n",
    "import unidecode\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GATHER DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../plot_sums/test\"\n",
    "\n",
    "movie_names = open(\"{}/movie_names.txt\".format(path))\n",
    "movie_names = movie_names.read().split(\",,,\")\n",
    "\n",
    "box_office = open(\"{}/box_office.txt\".format(path))\n",
    "box_office = box_office.read().split(\",,,\")\n",
    "\n",
    "imdbRating = open(\"{}/movie_ratings.txt\".format(path))\n",
    "imdbRating = imdbRating.read().split(\",,,\")\n",
    "\n",
    "metascore = open(\"{}/metascore.txt\".format(path))\n",
    "metascore = metascore.read().split(\",,,\")\n",
    "\n",
    "movie_runtime = open(\"{}/movie_runtime.txt\".format(path))\n",
    "movie_runtime = movie_runtime.read().split(\",,,\")\n",
    "real_runtimes = []\n",
    "for runtime in movie_runtime:\n",
    "    real_runtimes.append(runtime.replace(\" min\", \"\"))\n",
    "movie_runtime = real_runtimes\n",
    "\n",
    "movieBudget = open(\"{}/movieBudget.txt\".format(path))\n",
    "movieBudget = movieBudget.read().split(\",,,\")\n",
    "\n",
    "movieCast = open(\"{}/movie_cast.txt\".format(path))\n",
    "movieCast = movieCast.read().split(\",,,\")\n",
    "\n",
    "movie_directors = open(\"{}/directors.txt\".format(path))\n",
    "movie_directors = movie_directors.read().split(\",,,\")\n",
    "\n",
    "mpaa = open(\"{}/movie_mpaa.txt\".format(path))\n",
    "mpaa = mpaa.read().split(\",,,\")\n",
    "\n",
    "genre = open(\"{}/movie_genre.txt\".format(path))\n",
    "genre = genre.read().split(\",,,\")\n",
    "\n",
    "\n",
    "release = open(\"{}/releasedate.txt\".format(path))\n",
    "release = release.read().split(\",,,\")\n",
    "\n",
    "\n",
    "# ADD DIRECTOR, CAST, MPAA, IMDB RATING\n",
    "# features = [\"box_office\", \"imdb_rating\", \"metascore\", \"movie_mpaa\", \"movie_names\", \"movie_runtime\", \"movieBudget\"]\n",
    "\n",
    "# for i, feature in range(features):\n",
    "#     name = features[i]\n",
    "#     name = open(\"{}/{}.txt\".format(path, feature))\n",
    "#     feature = feature.read().split(\",,,\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies: 13000\n",
      "Box Office: 13000\n",
      "IMDb Rating: 13000\n",
      "Metascore: 13000\n",
      "Runtime: 13000\n",
      "Budget: 13000\n",
      "Movie Cast: 13000\n",
      "Movie Directors: 13000\n",
      "Genre: 13000\n",
      "MPAA: 13000\n",
      "Release Date: 13000\n"
     ]
    }
   ],
   "source": [
    "movie_names = movie_names[:-2001]\n",
    "print(\"Movies: {}\".format(len(movie_names)))\n",
    "\n",
    "box_office = box_office[:-2001]\n",
    "print(\"Box Office: {}\".format(len(box_office)))\n",
    "\n",
    "imdbRating = imdbRating[:-2001]\n",
    "print(\"IMDb Rating: {}\".format(len(imdbRating)))\n",
    "\n",
    "metascore = metascore[:-2001]\n",
    "print(\"Metascore: {}\".format(len(metascore)))\n",
    "\n",
    "movie_runtime = movie_runtime[:-2001]\n",
    "print(\"Runtime: {}\".format(len(movie_runtime)))\n",
    "\n",
    "movieBudget = movieBudget[:-2001]\n",
    "print(\"Budget: {}\".format(len(movieBudget)))\n",
    "\n",
    "movieCast = movieCast[:-2001]\n",
    "print(\"Movie Cast: {}\".format(len(movieCast)))\n",
    "\n",
    "movie_directors = movie_directors[:-2001]\n",
    "print(\"Movie Directors: {}\".format(len(movie_directors)))\n",
    "\n",
    "genre = genre[:-2001]\n",
    "genre_rating =[]\n",
    "for x in genre:\n",
    "    for idx, val in enumerate(set(genre)):\n",
    "        if x == val:\n",
    "            genre_rating.append(idx)\n",
    "genre = genre_rating\n",
    "print(\"Genre: {}\".format(len(genre)))\n",
    "\n",
    "mpaa = mpaa[:-2001]\n",
    "print(\"MPAA: {}\".format(len(mpaa)))\n",
    "\n",
    "release = release[:-2001]\n",
    "print(\"Release Date: {}\".format(len(release)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>box_office</th>\n",
       "      <th>rating</th>\n",
       "      <th>metascore</th>\n",
       "      <th>runtime</th>\n",
       "      <th>budget</th>\n",
       "      <th>genre</th>\n",
       "      <th>mpaa</th>\n",
       "      <th>cast</th>\n",
       "      <th>directors</th>\n",
       "      <th>release</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Star Wars: Episode VII - The Force Awakens</td>\n",
       "      <td>936662225</td>\n",
       "      <td>8.0</td>\n",
       "      <td>81</td>\n",
       "      <td>136</td>\n",
       "      <td>245000000</td>\n",
       "      <td>177</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>Harrison Ford,Carrie Fisher,Daisy Ridley,Oscar...</td>\n",
       "      <td>J.J. Abrams</td>\n",
       "      <td>18 December 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Avengers: Endgame</td>\n",
       "      <td>851612415</td>\n",
       "      <td>8.7</td>\n",
       "      <td>78</td>\n",
       "      <td>181</td>\n",
       "      <td>356000000</td>\n",
       "      <td>177</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>Robert Downey Jr.,Mark Ruffalo,Scarlett Johans...</td>\n",
       "      <td>Anthony Russo</td>\n",
       "      <td>26 April 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Avatar</td>\n",
       "      <td>760507625</td>\n",
       "      <td>7.8</td>\n",
       "      <td>83</td>\n",
       "      <td>162</td>\n",
       "      <td>237000000</td>\n",
       "      <td>205</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>Sam Worthington,Sigourney Weaver,Michelle Rodr...</td>\n",
       "      <td>James Cameron</td>\n",
       "      <td>18 December 2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Black Panther</td>\n",
       "      <td>700059566</td>\n",
       "      <td>7.3</td>\n",
       "      <td>88</td>\n",
       "      <td>134</td>\n",
       "      <td>200000000</td>\n",
       "      <td>177</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>Chadwick Boseman,Lupita Nyong'o,Martin Freeman...</td>\n",
       "      <td>Ryan Coogler</td>\n",
       "      <td>16 February 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Avengers: Infinity War</td>\n",
       "      <td>678815482</td>\n",
       "      <td>8.5</td>\n",
       "      <td>68</td>\n",
       "      <td>149</td>\n",
       "      <td>321000000</td>\n",
       "      <td>177</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>Robert Downey Jr.,Mark Ruffalo,Scarlett Johans...</td>\n",
       "      <td>Anthony Russo</td>\n",
       "      <td>27 April 2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         name box_office rating   metascore  \\\n",
       "0  Star Wars: Episode VII - The Force Awakens  936662225    8.0  81           \n",
       "1                           Avengers: Endgame  851612415    8.7  78           \n",
       "2                                      Avatar  760507625    7.8  83           \n",
       "3                               Black Panther  700059566    7.3  88           \n",
       "4                      Avengers: Infinity War  678815482    8.5  68           \n",
       "\n",
       "  runtime     budget  genre   mpaa  \\\n",
       "0     136  245000000    177  PG-13   \n",
       "1     181  356000000    177  PG-13   \n",
       "2     162  237000000    205  PG-13   \n",
       "3     134  200000000    177  PG-13   \n",
       "4     149  321000000    177  PG-13   \n",
       "\n",
       "                                                cast      directors  \\\n",
       "0  Harrison Ford,Carrie Fisher,Daisy Ridley,Oscar...    J.J. Abrams   \n",
       "1  Robert Downey Jr.,Mark Ruffalo,Scarlett Johans...  Anthony Russo   \n",
       "2  Sam Worthington,Sigourney Weaver,Michelle Rodr...  James Cameron   \n",
       "3  Chadwick Boseman,Lupita Nyong'o,Martin Freeman...   Ryan Coogler   \n",
       "4  Robert Downey Jr.,Mark Ruffalo,Scarlett Johans...  Anthony Russo   \n",
       "\n",
       "            release  \n",
       "0  18 December 2015  \n",
       "1     26 April 2019  \n",
       "2  18 December 2009  \n",
       "3  16 February 2018  \n",
       "4     27 April 2018  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'name': movie_names,\n",
    "    'box_office': box_office,\n",
    "    'rating': imdbRating,\n",
    "    'metascore': metascore,\n",
    "    'runtime': movie_runtime,\n",
    "    'budget': movieBudget,\n",
    "    'genre': genre,\n",
    "    'mpaa':mpaa,\n",
    "    'cast':movieCast,\n",
    "    'directors':movie_directors,\n",
    "    'release':release\n",
    "})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[df['budget'] == 'null'].index, inplace = True) \n",
    "df.drop(df[df['runtime'] == 'null'].index, inplace = True) \n",
    "df.drop(df[df['box_office'] == 'null'].index, inplace = True) \n",
    "df.drop(df[df['metascore'] == 'null'].index, inplace = True) \n",
    "df.drop(df[df['rating'] == 'null'].index, inplace = True) \n",
    "df.drop(df[df['mpaa'] == 'null'].index, inplace = True) \n",
    "df.drop(df[df['cast'] == 'null'].index, inplace = True) \n",
    "df.drop(df[df['directors'] == 'null'].index, inplace = True) \n",
    "df.drop(df[df['release'] == 'null'].index, inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpaa_rating =[]\n",
    "for x in df['mpaa']:\n",
    "    for idx, val in enumerate(set(df['mpaa'])):\n",
    "        if x == val:\n",
    "            mpaa_rating.append(idx)\n",
    "new_office = []\n",
    "for value in df['box_office']:\n",
    "    new_office.append(float(value))\n",
    "    \n",
    "new_budget = []\n",
    "for value in df['budget']:\n",
    "    new_budget.append(float(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: 5538\n",
      "BO: 5538\n",
      "Rating: 5538\n",
      "Meta: 5538\n",
      "Runtime: 5538\n",
      "Budget: 5538\n",
      "Genre: 5538\n",
      "Cast: 5538\n",
      "Directors: 5538\n",
      "Release: 5538\n"
     ]
    }
   ],
   "source": [
    "print(\"Name: {}\".format(len(df['name'])))\n",
    "print(\"BO: {}\".format(len(new_office)))\n",
    "print(\"Rating: {}\".format(len(df['rating'])))\n",
    "print(\"Meta: {}\".format(len(df['metascore'])))\n",
    "print(\"Runtime: {}\".format(len(df['runtime'])))\n",
    "print(\"Budget: {}\".format(len(df['budget'])))\n",
    "print(\"Genre: {}\".format(len(df['genre'])))\n",
    "print(\"Cast: {}\".format(len(df['cast'])))\n",
    "print(\"Directors: {}\".format(len(df['directors'])))\n",
    "print(\"Release: {}\".format(len(df['release'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5538"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'name': df['name'],\n",
    "    'box_office': new_office,\n",
    "    'rating': df['rating'],\n",
    "#     'metascore': df['metascore'],\n",
    "    'runtime': df['runtime'],\n",
    "    'budget': new_budget,\n",
    "    'genre': df['genre'],\n",
    "    'mpaa': mpaa_rating,\n",
    "    'cast':df['cast'],\n",
    "    'directors':df['directors'],\n",
    "    'release':df['release']\n",
    "})\n",
    "\n",
    "name = []\n",
    "box_office = []\n",
    "rating =[]\n",
    "runtime = []\n",
    "budget = []\n",
    "genre = []\n",
    "mpaa = []\n",
    "cast = []\n",
    "directors = []\n",
    "release = []\n",
    "\n",
    "for val in df['name']:\n",
    "  name.append(val)\n",
    "for val in df['box_office']:\n",
    "  box_office.append(val)\n",
    "for val in df['rating']:\n",
    "  rating.append(val)\n",
    "for val in df['runtime']:\n",
    "  runtime.append(val)\n",
    "for val in df['budget']:\n",
    "  budget.append(val)\n",
    "for val in df['genre']:\n",
    "  genre.append(val)\n",
    "for val in df['mpaa']:\n",
    "  mpaa.append(val)\n",
    "for val in df['cast']:\n",
    "  cast.append(val)\n",
    "for val in df['directors']:\n",
    "  directors.append(val)\n",
    "for val in df['release']:\n",
    "    release.append(val)\n",
    "\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n"
     ]
    }
   ],
   "source": [
    "def actor_gross(actors):\n",
    "    avg_gross_act = []\n",
    "    for actor in actors:\n",
    "        # CALCULATE TOTAL MOVIE GROSS\n",
    "#         avg_gross_act.append(df.loc[df['cast'].astype(str).str.contains(actor), 'box_office'].sum())\n",
    "        total_gross = df.loc[df['cast'].astype(str).str.contains(actor), 'box_office'].sum()\n",
    "        total_movies = len(df.loc[df['cast'].astype(str).str.contains(actor)])\n",
    "        avg_gross_act.append(total_gross/total_movies)\n",
    "    return avg_gross_act\n",
    "\n",
    "def director_gross(directors):\n",
    "    avg_gross_dir = []\n",
    "    for director in directors:\n",
    "        # CALCULATE TOTAL MOVIE GROSS\n",
    "#         avg_gross_act.append(df.loc[df['cast'].astype(str).str.contains(actor), 'box_office'].sum())\n",
    "        total_gross = df.loc[df['directors'].astype(str).str.contains(director), 'box_office'].sum()\n",
    "        total_movies = len(df.loc[df['directors'].astype(str).str.contains(director)])\n",
    "        avg_gross_dir.append(total_gross/total_movies)\n",
    "    return avg_gross_dir\n",
    "\n",
    "\n",
    "first_five = []\n",
    "for cast in df['cast']:\n",
    "    cast = cast.split(\",\")\n",
    "    top_five = cast[:5]\n",
    "    top_five = \",\".join(top_five)\n",
    "    first_five.append(top_five)\n",
    "\n",
    "actor_avg = []\n",
    "counter = 0\n",
    "for cast in first_five:\n",
    "    counter += 1\n",
    "    members = cast.split(\",\")\n",
    "    actor_avg.append(actor_gross(members))\n",
    "    if counter % 500 == 0:\n",
    "        print(counter)\n",
    "\n",
    "weighted_actors = []\n",
    "for val in actor_avg:\n",
    "    val = sum(val)/len(val)\n",
    "    val = val*.7\n",
    "    weighted_actors.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n",
      "10000\n",
      "10500\n",
      "11000\n"
     ]
    }
   ],
   "source": [
    "dir_avg = []\n",
    "for cast in df['directors']:\n",
    "    counter +=1\n",
    "    members = cast.split(\",\")\n",
    "    dir_avg.append(director_gross(members))\n",
    "    if counter % 500 == 0:\n",
    "        print(counter)\n",
    "\n",
    "weighted_directors = []\n",
    "for val in dir_avg:\n",
    "    val = sum(val)/len(val)\n",
    "    val = val * .3\n",
    "    weighted_directors.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "star_feature = pd.DataFrame({\n",
    "    'actor': weighted_actors,\n",
    "    'director': weighted_directors,\n",
    "})\n",
    "\n",
    "star_feature['star_value'] = star_feature['actor'] + star_feature['director']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = []\n",
    "for date in df['release']:\n",
    "    if \"January\" in date:\n",
    "        months.append(\"January\")\n",
    "    elif \"February\" in date:\n",
    "        months.append(\"February\")\n",
    "    elif \"March\" in date:\n",
    "        months.append(\"March\")\n",
    "    elif \"April\" in date:\n",
    "        months.append(\"April\")\n",
    "    elif \"May\" in date:\n",
    "        months.append(\"May\")\n",
    "    elif \"June\" in date:\n",
    "        months.append(\"June\")\n",
    "    elif \"July\" in date:\n",
    "        months.append(\"July\")\n",
    "    elif \"August\" in date:\n",
    "        months.append(\"August\")\n",
    "    elif \"September\" in date:\n",
    "        months.append(\"September\")\n",
    "    elif \"October\" in date:\n",
    "        months.append(\"October\")\n",
    "    elif \"November\" in date:\n",
    "        months.append(\"November\")\n",
    "    elif \"December\" in date:\n",
    "        months.append(\"December\")\n",
    "    else:\n",
    "        months.append(\"null\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR null\n",
      "ERROR null\n",
      "ERROR null\n",
      "ERROR null\n",
      "ERROR null\n",
      "ERROR null\n",
      "ERROR null\n",
      "ERROR null\n"
     ]
    }
   ],
   "source": [
    "comp_high = []\n",
    "comp_med = []\n",
    "comp_low = []\n",
    "\n",
    "for comp in months:\n",
    "    if comp in [\"January\", \"August\", \"September\", \"October\"]:\n",
    "        comp_high.append(1)\n",
    "        comp_med.append(0)\n",
    "        comp_low.append(0)\n",
    "    elif comp in [\"February\", \"March\", \"April\", \"November\", \"December\"]:\n",
    "        comp_high.append(0)\n",
    "        comp_med.append(1)\n",
    "        comp_low.append(0)\n",
    "    elif comp in [\"May\", \"June\", \"July\"]:\n",
    "        comp_high.append(0)\n",
    "        comp_med.append(0)\n",
    "        comp_low.append(1)\n",
    "    else:\n",
    "        print(\"ERROR\", comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "arrays must all be same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-123-4d491f2be1e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;34m'comp_high'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcomp_high\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;34m'comp_med'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcomp_med\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[1;34m'comp_low'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcomp_low\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m })\n\u001b[0;32m     16\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'release'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'null'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\brian\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    346\u001b[0m                                  dtype=dtype, copy=copy)\n\u001b[0;32m    347\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 348\u001b[1;33m             \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    349\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\brian\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_init_dict\u001b[1;34m(self, data, index, columns, dtype)\u001b[0m\n\u001b[0;32m    457\u001b[0m             \u001b[0marrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 459\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_arrays_to_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_init_ndarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\brian\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_arrays_to_mgr\u001b[1;34m(arrays, arr_names, index, columns, dtype)\u001b[0m\n\u001b[0;32m   7354\u001b[0m     \u001b[1;31m# figure out the index, if necessary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7355\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7356\u001b[1;33m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7357\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7358\u001b[0m     \u001b[1;31m# don't force copy because getting jammed in an ndarray anyway\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\brian\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mextract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   7400\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7401\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7402\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'arrays must all be same length'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7404\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: arrays must all be same length"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'name': name,\n",
    "    'box_office': box_office,\n",
    "    'rating': rating,\n",
    "#     'metascore': df['metascore'],\n",
    "    'runtime': runtime,\n",
    "    'budget': budget,\n",
    "    'genre': genre,\n",
    "    'mpaa': mpaa,\n",
    "    'cast':cast,\n",
    "    'star_val':star_feature['star_value'],\n",
    "    'comp_high': comp_high,\n",
    "    'comp_med': comp_med,\n",
    "    'comp_low': comp_low\n",
    "})\n",
    "df.drop(df[df['release'] == 'null'].index, inplace = True) \n",
    "print(len(df))\n",
    "# print(\"Name: {}\".len(df['name']))\n",
    "# print(\"Box Office: {}\".len(df['box_office']))\n",
    "# print(\"Rating: {}\".len(df['rating']))\n",
    "# print(\"Runtime: {}\".len(df['runtime']))\n",
    "# print(\"Budget: {}\".len(df['budget']))\n",
    "# print(\"Genre: {}\".len(df['genre']))\n",
    "# print(\"MPAA: {}\".len(df['mpaa']))\n",
    "# print(\"Cast: {}\".len(df['cast']))\n",
    "# print(\"Star Val: {}\".len(df['star_val']))\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>box_office</th>\n",
       "      <th>rating</th>\n",
       "      <th>runtime</th>\n",
       "      <th>budget</th>\n",
       "      <th>genre</th>\n",
       "      <th>mpaa</th>\n",
       "      <th>cast</th>\n",
       "      <th>directors</th>\n",
       "      <th>release</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Star Wars: Episode VII - The Force Awakens</td>\n",
       "      <td>936662225.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>136</td>\n",
       "      <td>245000000.0</td>\n",
       "      <td>177</td>\n",
       "      <td>11</td>\n",
       "      <td>Harrison Ford,Carrie Fisher,Daisy Ridley,Oscar...</td>\n",
       "      <td>J.J. Abrams</td>\n",
       "      <td>18 December 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Avengers: Endgame</td>\n",
       "      <td>851612415.0</td>\n",
       "      <td>8.7</td>\n",
       "      <td>181</td>\n",
       "      <td>356000000.0</td>\n",
       "      <td>177</td>\n",
       "      <td>11</td>\n",
       "      <td>Robert Downey Jr.,Mark Ruffalo,Scarlett Johans...</td>\n",
       "      <td>Anthony Russo</td>\n",
       "      <td>26 April 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Avatar</td>\n",
       "      <td>760507625.0</td>\n",
       "      <td>7.8</td>\n",
       "      <td>162</td>\n",
       "      <td>237000000.0</td>\n",
       "      <td>205</td>\n",
       "      <td>11</td>\n",
       "      <td>Sam Worthington,Sigourney Weaver,Michelle Rodr...</td>\n",
       "      <td>James Cameron</td>\n",
       "      <td>18 December 2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Black Panther</td>\n",
       "      <td>700059566.0</td>\n",
       "      <td>7.3</td>\n",
       "      <td>134</td>\n",
       "      <td>200000000.0</td>\n",
       "      <td>177</td>\n",
       "      <td>11</td>\n",
       "      <td>Chadwick Boseman,Lupita Nyong'o,Martin Freeman...</td>\n",
       "      <td>Ryan Coogler</td>\n",
       "      <td>16 February 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Avengers: Infinity War</td>\n",
       "      <td>678815482.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>149</td>\n",
       "      <td>321000000.0</td>\n",
       "      <td>177</td>\n",
       "      <td>11</td>\n",
       "      <td>Robert Downey Jr.,Mark Ruffalo,Scarlett Johans...</td>\n",
       "      <td>Anthony Russo</td>\n",
       "      <td>27 April 2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         name   box_office rating runtime  \\\n",
       "0  Star Wars: Episode VII - The Force Awakens  936662225.0    8.0     136   \n",
       "1                           Avengers: Endgame  851612415.0    8.7     181   \n",
       "2                                      Avatar  760507625.0    7.8     162   \n",
       "3                               Black Panther  700059566.0    7.3     134   \n",
       "4                      Avengers: Infinity War  678815482.0    8.5     149   \n",
       "\n",
       "        budget  genre  mpaa  \\\n",
       "0  245000000.0    177    11   \n",
       "1  356000000.0    177    11   \n",
       "2  237000000.0    205    11   \n",
       "3  200000000.0    177    11   \n",
       "4  321000000.0    177    11   \n",
       "\n",
       "                                                cast      directors  \\\n",
       "0  Harrison Ford,Carrie Fisher,Daisy Ridley,Oscar...    J.J. Abrams   \n",
       "1  Robert Downey Jr.,Mark Ruffalo,Scarlett Johans...  Anthony Russo   \n",
       "2  Sam Worthington,Sigourney Weaver,Michelle Rodr...  James Cameron   \n",
       "3  Chadwick Boseman,Lupita Nyong'o,Martin Freeman...   Ryan Coogler   \n",
       "4  Robert Downey Jr.,Mark Ruffalo,Scarlett Johans...  Anthony Russo   \n",
       "\n",
       "            release  \n",
       "0  18 December 2015  \n",
       "1     26 April 2019  \n",
       "2  18 December 2009  \n",
       "3  16 February 2018  \n",
       "4     27 April 2018  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[\"runtime\", \"rating\", \"budget\", \"genre\", \"mpaa\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_regression = df['box_office']\n",
    "y_train_regression= y_train_regression.astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\brian\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "c:\\users\\brian\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "c:\\users\\brian\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "c:\\users\\brian\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "c:\\users\\brian\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>runtime</th>\n",
       "      <th>rating</th>\n",
       "      <th>budget</th>\n",
       "      <th>genre</th>\n",
       "      <th>mpaa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>136.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>245000000.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>181.0</td>\n",
       "      <td>8.7</td>\n",
       "      <td>356000000.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>162.0</td>\n",
       "      <td>7.8</td>\n",
       "      <td>237000000.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>134.0</td>\n",
       "      <td>7.3</td>\n",
       "      <td>200000000.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>149.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>321000000.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   runtime  rating       budget  genre  mpaa\n",
       "0    136.0     8.0  245000000.0  177.0  11.0\n",
       "1    181.0     8.7  356000000.0  177.0  11.0\n",
       "2    162.0     7.8  237000000.0  205.0  11.0\n",
       "3    134.0     7.3  200000000.0  177.0  11.0\n",
       "4    149.0     8.5  321000000.0  177.0  11.0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['runtime'] = X['runtime'].astype(np.float)\n",
    "X['rating'] = X['rating'].astype(np.float)\n",
    "X['budget'] = X['budget'].astype(np.float)\n",
    "X['genre'] = X['genre'].astype(np.float)\n",
    "# X['budget'] = X['budget'].astype(np.float)\n",
    "X['mpaa'] = X['mpaa'].astype(np.float)\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    936662225.0\n",
       "1    851612415.0\n",
       "2    760507625.0\n",
       "3    700059566.0\n",
       "4    678815482.0\n",
       "Name: box_office, dtype: float64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_regression.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "for value in y_train_regression:\n",
    "    value = float(value)\n",
    "#     print (type(value))\n",
    "    if value <= 1000000:\n",
    "        y.append(0)\n",
    "    elif value > 1000000 and value <= 10000000:\n",
    "        y.append(1)\n",
    "    elif value > 10000000 and value <= 25000000:\n",
    "        y.append(2)\n",
    "    elif value > 25000000 and value <= 50000000:\n",
    "        y.append(3)\n",
    "    elif value > 50000000 and value <= 100000000:\n",
    "        y.append(4)\n",
    "    elif value > 100000000 and value <= 200000000:\n",
    "        y.append(5)\n",
    "    elif value > 200000000 and value <= 300000000:\n",
    "        y.append(6)\n",
    "    elif value > 300000000 and value <= 400000000:\n",
    "        y.append(7)\n",
    "    elif value > 400000000 and value <= 500000000:\n",
    "        y.append(8)\n",
    "    elif value > 500000000:\n",
    "        y.append(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "X_df = X\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "x_temp = scaler.transform(X)\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(x_temp, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.53094899,  1.57865845,  5.13967361, -0.43318182,  1.33340096],\n",
       "       [ 3.96656964,  2.27535217,  7.83331668, -0.43318182,  1.33340096],\n",
       "       [ 2.93819648,  1.3796031 ,  4.94553717, -0.2530782 ,  1.33340096],\n",
       "       [ 1.42269918,  0.88196472,  4.04765615, -0.43318182,  1.33340096],\n",
       "       [ 2.23457273,  2.07629682,  6.98396976, -0.43318182,  1.33340096]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_temp[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Create tensors for our train and test set. \n",
    "As you remember we need variable to accumulate gradients. \n",
    "Therefore first we create tensor, then we will create variable '''\n",
    "# Numpy to Tensor Conversion (Train Set)\n",
    "X_train = torch.from_numpy(X_train)\n",
    "y_train = torch.from_numpy(y_train)\n",
    "\n",
    "# Numpy to Tensor Conversion (Train Set)\n",
    "X_test = torch.from_numpy(X_test)\n",
    "y_test = torch.from_numpy(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make torch datasets from train and test sets\n",
    "train = torch.utils.data.TensorDataset(X_train,y_train)\n",
    "test = torch.utils.data.TensorDataset(X_test,y_test)\n",
    "\n",
    "# Create train and test data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size = 64, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size = 64, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.modules.batchnorm as bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN(nn.Module):\n",
    "    def __init__(self, input_dim = 5, output_dim = 10):\n",
    "        super(ANN, self).__init__()\n",
    "    \n",
    "        # Input Layer (2) -> 784\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        # 256 -> 128\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        # 128 -> 128\n",
    "        self.fc3 = nn.Linear(128, 128)\n",
    "        # 128 -> 64\n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        # 64 -> 64\n",
    "        self.fc5 = nn.Linear(64, 64)\n",
    "        # 64 -> 32\n",
    "        self.fc6 = nn.Linear(64, 32)\n",
    "        # 32 -> 32\n",
    "        self.fc7 = nn.Linear(32, 32)\n",
    "        # 32 -> output layer(10)\n",
    "        self.output_layer = nn.Linear(32,10)\n",
    "        # Dropout Layer (20%) to reduce overfitting\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(5)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(128)\n",
    "    \n",
    "    # Feed Forward Function\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # csv input\n",
    "        x = x.view(-1, 5)\n",
    "        x = self.batchnorm1(x)\n",
    "        # Add ReLU activation function to each layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        # Add dropout layer\n",
    "        #x = self.dropout(x)\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc6(x))\n",
    "        x = F.relu(self.fc7(x))\n",
    "        x = self.dropout(x)\n",
    "        # Don't add any ReLU activation function to Last Output Layer\n",
    "        x = self.output_layer(x)\n",
    "        \n",
    "        # Return the created model\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANN(\n",
      "  (fc1): Linear(in_features=5, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (fc4): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc5): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc6): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc7): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (output_layer): Linear(in_features=32, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.2)\n",
      "  (batchnorm1): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batchnorm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create the Neural Network Model\n",
    "model = ANN(input_dim = 5, output_dim = 10)\n",
    "# Print its architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "# specify loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 9028.4260\t Val. acc: 24.00%\n",
      "Validation loss decreased (inf --> 2010.364628).  Saving model ...\n",
      "Epoch: 51 \tTraining Loss: 6676.7237\t Val. acc: 35.00%\n",
      "Validation loss decreased (2010.364628 --> 1732.867343).  Saving model ...\n",
      "Epoch: 101 \tTraining Loss: 6509.3147\t Val. acc: 36.00%\n",
      "Validation loss decreased (1732.867343 --> 1718.439987).  Saving model ...\n",
      "Epoch: 151 \tTraining Loss: 6383.0450\t Val. acc: 34.00%\n",
      "Epoch: 201 \tTraining Loss: 6281.6075\t Val. acc: 35.00%\n",
      "Epoch: 251 \tTraining Loss: 6109.5739\t Val. acc: 34.00%\n",
      "Epoch: 301 \tTraining Loss: 5976.5691\t Val. acc: 32.00%\n",
      "Epoch: 351 \tTraining Loss: 5898.6987\t Val. acc: 34.00%\n",
      "Epoch: 401 \tTraining Loss: 5821.1662\t Val. acc: 32.00%\n",
      "Epoch: 451 \tTraining Loss: 5654.1307\t Val. acc: 33.00%\n"
     ]
    }
   ],
   "source": [
    "# Define epochs (between 20-50)\n",
    "epochs = 500\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "# Some lists to keep track of loss and accuracy during each epoch\n",
    "epoch_list = []\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "train_acc_list = []\n",
    "val_acc_list = []\n",
    "\n",
    "# Start epochs\n",
    "for epoch in range(epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    # Set the training mode ON -> Activate Dropout Layers\n",
    "    model.train() # prepare model for training\n",
    "    # Calculate Accuracy         \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Load Train Images with Labels(Targets)\n",
    "    for data, target in train_loader:\n",
    "        \n",
    "        # Convert our feature and labels to Variables to accumulate Gradients\n",
    "        data = Variable(data).float()\n",
    "        target = Variable(target).type(torch.LongTensor)\n",
    "        \n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        \n",
    "        # Calculate Training Accuracy \n",
    "        predicted = torch.max(output.data, 1)[1]        \n",
    "        # Total number of labels\n",
    "        total += len(target)\n",
    "        # Total correct predictions\n",
    "        correct += (predicted == target).sum()\n",
    "        \n",
    "        # calculate the loss\n",
    "        loss = loss_fn(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "    \n",
    "    # calculate average training loss over an epoch\n",
    "    train_loss = np.mean(train_loss)\n",
    "    \n",
    "    # Avg Accuracy\n",
    "    accuracy = 100 * correct / float(total)\n",
    "    \n",
    "    # Put them in their list\n",
    "    train_acc_list.append(accuracy)\n",
    "    train_loss_list.append(train_loss)\n",
    "    \n",
    "        \n",
    "    # Implement Validation like K-fold Cross-validation \n",
    "    # Set Evaluation Mode ON -> Turn Off Dropout\n",
    "    model.eval() # Required for Evaluation/Test\n",
    "\n",
    "    # Calculate Test/Validation Accuracy         \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "\n",
    "            # Convert our images and labels to Variables to accumulate Gradients\n",
    "            data = Variable(data).float()\n",
    "            target = Variable(target).type(torch.LongTensor)\n",
    "\n",
    "            # Predict Output\n",
    "            output = model(data)\n",
    "\n",
    "            # Calculate Loss\n",
    "            loss = loss_fn(output, target)\n",
    "            val_loss += loss.item()*data.size(0)\n",
    "            # Get predictions from the maximum value\n",
    "            predicted = torch.max(output.data, 1)[1]\n",
    "\n",
    "            # Total number of labels\n",
    "            total += len(target)\n",
    "\n",
    "            # Total correct predictions\n",
    "            correct += (predicted == target).sum()\n",
    "    \n",
    "    # calculate average training loss and accuracy over an epoch\n",
    "    val_loss = np.mean(val_loss)\n",
    "    accuracy = 100 * correct/ float(total)\n",
    "    \n",
    "    # Put them in their list\n",
    "    val_acc_list.append(accuracy)\n",
    "    val_loss_list.append(val_loss)\n",
    "    if epoch % 50 == 0:\n",
    "        # Print the Epoch and Training Loss Details with Validation Accuracy   \n",
    "        print('Epoch: {} \\tTraining Loss: {:.4f}\\t Val. acc: {:.2f}%'.format(\n",
    "            epoch+1, \n",
    "            train_loss,\n",
    "            accuracy\n",
    "            ))\n",
    "        # save model if validation loss has decreased\n",
    "        if val_loss <= valid_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "            valid_loss_min,\n",
    "            val_loss))\n",
    "            torch.save(model.state_dict(), 'model.pt')\n",
    "            valid_loss_min = val_loss\n",
    "        # Move to next epoch\n",
    "        epoch_list.append(epoch + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.load(\"model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 7239.0473\t Val. acc: 0.00%\n",
      "Validation loss decreased (inf --> 4513.914797).  Saving model ...\n",
      "Epoch: 51 \tTraining Loss: 5497.9877\t Val. acc: 8.00%\n",
      "Epoch: 1 \tTraining Loss: 7168.1910\t Val. acc: 17.00%\n",
      "Validation loss decreased (inf --> 1933.014946).  Saving model ...\n",
      "Epoch: 51 \tTraining Loss: 5749.3149\t Val. acc: 5.00%\n",
      "Epoch: 1 \tTraining Loss: 6022.3080\t Val. acc: 12.00%\n",
      "Validation loss decreased (inf --> 1960.617946).  Saving model ...\n",
      "Epoch: 51 \tTraining Loss: 5615.8385\t Val. acc: 6.00%\n",
      "Epoch: 1 \tTraining Loss: 6115.5569\t Val. acc: 13.00%\n",
      "Validation loss decreased (inf --> 1996.993006).  Saving model ...\n",
      "Epoch: 51 \tTraining Loss: 5588.7773\t Val. acc: 7.00%\n",
      "Epoch: 1 \tTraining Loss: 6383.2516\t Val. acc: 14.00%\n",
      "Validation loss decreased (inf --> 3955.241919).  Saving model ...\n",
      "Epoch: 51 \tTraining Loss: 5592.4107\t Val. acc: 10.00%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "x_temp = scaler.transform(X)\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(x_temp, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "kfolds = KFold(5, False).split(x_temp)\n",
    "for _, (X_index, y_index) in enumerate(kfolds):\n",
    "    '''Create tensors for our train and test set. \n",
    "    As you remember we need variable to accumulate gradients. \n",
    "    Therefore first we create tensor, then we will create variable '''\n",
    "    X_train = X[X_index]\n",
    "    y_train = y[X_index]\n",
    "    X_test = X[y_index]\n",
    "    y_test = y[y_index]\n",
    "    X_train = torch.from_numpy(X_train)\n",
    "    y_train = torch.from_numpy(y_train)\n",
    "\n",
    "    # Numpy to Tensor Conversion (Train Set)\n",
    "    X_test = torch.from_numpy(X_test)\n",
    "    y_test = torch.from_numpy(y_test)\n",
    "    # Make torch datasets from train and test sets\n",
    "    train = torch.utils.data.TensorDataset(X_train,y_train)\n",
    "    test = torch.utils.data.TensorDataset(X_test,y_test)\n",
    "\n",
    "    # Create train and test data loaders\n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size = 64, shuffle = True)\n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size = 64, shuffle = True)\n",
    "    # Define 1epochs (between 20-50)\n",
    "    epochs = 51\n",
    "\n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "    # Some lists to keep track of loss and accuracy during each epoch\n",
    "    epoch_list = []\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    train_acc_list = []\n",
    "    val_acc_list = []\n",
    "\n",
    "    # Start epochs\n",
    "    for epoch in range(epochs):\n",
    "        # monitor training loss\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        # Set the training mode ON -> Activate Dropout Layers\n",
    "        model.train() # prepare model for training\n",
    "        # Calculate Accuracy         \n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # Load Train Images with Labels(Targets)\n",
    "        for data, target in train_loader:\n",
    "\n",
    "            # Convert our feature and labels to Variables to accumulate Gradients\n",
    "            data = Variable(data).float()\n",
    "            target = Variable(target).type(torch.LongTensor)\n",
    "\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "\n",
    "            # Calculate Training Accuracy \n",
    "            predicted = torch.max(output.data, 1)[1]        \n",
    "            # Total number of labels\n",
    "            total += len(target)\n",
    "            # Total correct predictions\n",
    "            correct += (predicted == target).sum()\n",
    "\n",
    "            # calculate the loss\n",
    "            loss = loss_fn(output, target)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update running training loss\n",
    "            train_loss += loss.item()*data.size(0)\n",
    "\n",
    "        # calculate average training loss over an epoch\n",
    "        train_loss = np.mean(train_loss)\n",
    "\n",
    "        # Avg Accuracy\n",
    "        accuracy = 100 * correct / float(total)\n",
    "\n",
    "        # Put them in their list\n",
    "        train_acc_list.append(accuracy)\n",
    "        train_loss_list.append(train_loss)\n",
    "\n",
    "\n",
    "        # Implement Validation like K-fold Cross-validation \n",
    "        # Set Evaluation Mode ON -> Turn Off Dropout\n",
    "        model.eval() # Required for Evaluation/Test\n",
    "\n",
    "        # Calculate Test/Validation Accuracy         \n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "\n",
    "                # Convert our images and labels to Variables to accumulate Gradients\n",
    "                data = Variable(data).float()\n",
    "                target = Variable(target).type(torch.LongTensor)\n",
    "\n",
    "                # Predict Output\n",
    "                output = model(data)\n",
    "\n",
    "                # Calculate Loss\n",
    "                loss = loss_fn(output, target)\n",
    "                val_loss += loss.item()*data.size(0)\n",
    "                # Get predictions from the maximum value\n",
    "                predicted = torch.max(output.data, 1)[1]\n",
    "\n",
    "                # Total number of labels\n",
    "                total += len(target)\n",
    "\n",
    "                # Total correct predictions\n",
    "                correct += (predicted == target).sum()\n",
    "\n",
    "        # calculate average training loss and accuracy over an epoch\n",
    "        val_loss = np.mean(val_loss)\n",
    "        accuracy = 100 * correct/ float(total)\n",
    "\n",
    "        # Put them in their list\n",
    "        val_acc_list.append(accuracy)\n",
    "        val_loss_list.append(val_loss)\n",
    "        if epoch % 50 == 0:\n",
    "            # Print the Epoch and Training Loss Details with Validation Accuracy   \n",
    "            print('Epoch: {} \\tTraining Loss: {:.4f}\\t Val. acc: {:.2f}%'.format(\n",
    "                epoch+1, \n",
    "                train_loss,\n",
    "                accuracy\n",
    "                ))\n",
    "            # save model if validation loss has decreased\n",
    "            if val_loss <= valid_loss_min:\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "                valid_loss_min,\n",
    "                val_loss))\n",
    "                torch.save(model.state_dict(), 'model.pt')\n",
    "                valid_loss_min = val_loss\n",
    "            # Move to next epoch\n",
    "            epoch_list.append(epoch + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\brian\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=5, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(x_temp, y)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1ae1fdd9c88>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "std = np.std([tree.feature_importances_ for tree in clf.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)\n",
    "\n",
    "(pd.Series(clf.feature_importances_, index=X_df.columns)\n",
    "   .nlargest(5)\n",
    "   .plot(kind='barh'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
